# 微信爬虫项目总结

## 项目概述

这是一个基于微信公众号后台API的自动化爬虫系统，支持多公众号监控、自动登录、增量爬取和多格式导出。

## 项目位置

```
C:\Dev\PythonBox\wechat_crawler\
```

## 核心功能

* ✅ **全自动登录**：扫码一次，2天内免登录

* ✅ **自动获取fakeid**：自动搜索并获取公众号唯一标识

* ✅ **增量爬取**：只获取新文章，自动跳过已存在的文章

* ✅ **多公众号监控**：支持同时监控多个公众号

* ✅ **多格式导出**：JSON/CSV/Excel/Word（按公众号分类）

* ✅ **数据持久化**：SQLite数据库存储，支持历史数据查询

## 文件结构

```
wechat_crawler/
├── config/                          # 配置文件目录
│   ├── wechat_accounts.json         # 监控的公众号列表（包含fakeid，永久有效）
│   └── wechat_session.json          # 登录会话信息（2天有效）
├── data/
│   ├── db/
│   │   └── wechat.db               # SQLite数据库（存储所有文章）
│   └── processed/                   # 导出文件目录
│       ├── 公众号名称/              # 按公众号分类的Word文件
│       │   └── 20260214_标题.docx
│       ├── all_articles_20260214.json
│       └── all_articles_20260214.xlsx
├── logs/
│   └── wechat_crawler.log          # 运行日志
├── modules/                         # 核心模块
│   ├── __init__.py
│   ├── crawler.py                   # 原有搜狗爬虫（已弃用）
│   ├── storage.py                   # 数据库存储模块
│   ├── wechat_api_crawler.py       # 微信公众号API爬虫
│   ├── wechat_auto_login_simple.py # 自动登录模块
│   └── word_exporter.py            # Word导出模块
└── scripts/
    └── wechat_crawler_main.py      # 主程序入口
```

## 使用方法

### 1. 运行程序

```bash
cd C:\Dev\PythonBox\wechat_crawler
python scripts/wechat_crawler_main.py
```

### 2. 首次使用流程

1. **添加监控公众号**：输入公众号名称（支持逗号分隔多个）
2. **扫码登录**：程序自动打开浏览器，显示二维码，使用微信扫码
3. **自动获取fakeid**：程序自动搜索并获取公众号fakeid
4. **自动爬取文章**：获取文章列表并保存到数据库
5. **选择导出格式**：JSON/CSV/Excel/Word

### 3. 后续使用

* 直接运行程序，自动使用已保存的登录信息（免扫码）

* 自动增量爬取，只获取新文章

* 支持导出所有历史文章为Word

## 导出选项

1. **JSON** - 导出为JSON格式
2. **CSV** - 导出为CSV格式
3. **Excel** - 导出为Excel格式
4. **Word文档（仅新文章）** - 只导出本次爬取的新文章
5. **Word文档（所有文章）** - 导出数据库中所有文章
6. **不导出** - 跳过导出步骤

## 关键配置信息

### 已监控的公众号

* **丹湖渔翁** - fakeid: `MzAwNjAwNTc3Ng==`

* **人民日报** - （之前测试添加的）

### 数据库状态

* 共保存 **26篇文章**

* 支持增量爬取，自动去重

* 数据库位置：`data/db/wechat.db`

## 依赖库

```bash
pip install selenium requests pandas python-docx
```

## 核心模块说明

### 1. wechat\_auto\_login\_simple.py

* 自动登录微信公众号后台

* 保存和加载会话信息

* 自动获取公众号fakeid

### 2. wechat\_api\_crawler.py

* 调用微信公众号后台API

* 获取文章列表

* 支持翻页获取历史文章

### 3. storage.py

* SQLite数据库操作

* 文章保存和查询

* 增量爬取支持（检查文章是否已存在）

* 数据导出功能（JSON/CSV/Excel）

### 4. word\_exporter.py

* 按公众号分类创建文件夹

* 每篇文章导出为单独的Word文档

* 文件名格式：`日期_文章标题.docx`

## 注意事项

1. **登录有效期**：2天，过期后需要重新扫码登录
2. **fakeid有效期**：永久有效，只需获取一次
3. **增量爬取**：自动检测已存在的文章，避免重复保存
4. **Word导出**：按公众号名称创建文件夹，便于管理

## 数据流程

```
用户运行程序
    ↓
加载监控列表（config/wechat_accounts.json）
    ↓
检查登录状态（config/wechat_session.json）
    ↓
如果需要：扫码登录 → 保存会话
    ↓
检查fakeid
    ↓
如果需要：自动获取fakeid → 保存到配置文件
    ↓
爬取文章
    ↓
检查文章是否已存在（增量爬取）
    ↓
保存新文章到数据库（data/db/wechat.db）
    ↓
导出数据（可选）
    ↓
Word导出：按公众号分类创建文件夹
```

## 可能的后续优化

1. **定时任务**：添加自动定期爬取功能
2. **通知功能**：有新文章时发送邮件/微信通知
3. **全文抓取**：获取文章完整内容（目前只有摘要）
4. **图片下载**：下载文章中的图片
5. **代理支持**：添加代理池提高稳定性
6. **Web界面**：开发Web管理界面
7. **多账号支持**：支持多个微信公众号登录

## 快速开始

```bash
# 进入项目目录
cd C:\Dev\PythonBox\wechat_crawler

# 运行爬虫
python scripts/wechat_crawler_main.py
```

## 常见问题

### Q: 为什么需要扫码登录？

A: 微信没有开放API，必须通过浏览器模拟登录获取token和cookie。

### Q: fakeid是什么？

A: 公众号的唯一标识符，类似于身份证号，每个公众号固定不变。

### Q: 为什么有时候获取0篇文章？

A: 增量爬取机制，如果数据库中已有这些文章，会自动跳过。

### Q: 如何添加新的公众号？

A: 运行程序时选择"添加新的公众号"，输入公众号名称即可。

### Q: Word文件保存在哪里？

A: `data/processed/公众号名称/` 目录下。

## 项目状态

**已完成所有核心功能，可以正常使用！** 🎉

* ✅ 自动登录

* ✅ 自动获取fakeid

* ✅ 增量爬取

* ✅ 多格式导出

* ✅ 按公众号分类存储

