# 微信公众号推送提取项目方案

## 🎯 项目目标

创建一个自动化工具，用于：
1. **单公众号提取**：用户输入微信公众号名称/ID，自动爬取所有历史推送
2. **多公众号管理**：根据用户提供的名单，创建公众号列表
3. **定期更新**：定期检查并拉取新的推送
4. **数据存储**：保存爬取的文章内容、标题、发布时间等信息

## 📋 技术栈

| 技术 | 版本 | 用途 |
|------|------|------|
| Python | 3.13+ | 核心语言 |
| requests | 2.32+ | HTTP 请求 |
| beautifulsoup4 | 4.12+ | HTML 解析 |
| selenium | 4.16+ | 处理动态内容（备用） |
| pandas | 2.2+ | 数据处理和存储 |
| schedule | 1.2+ | 定时任务 |
| sqlite3 | 内置 | 本地数据库存储 |
| tkinter | 内置 | 图形界面（可选） |

## 📁 项目结构

```
wechat_crawler/
├── data/
│   ├── raw/            # 原始爬取数据
│   ├── processed/       # 处理后的数据
│   └── db/              # SQLite 数据库
├── modules/
│   ├── crawler.py       # 爬虫核心模块
│   ├── scheduler.py     # 定时任务模块
│   ├── storage.py       # 数据存储模块
│   └── gui.py           # 图形界面模块
├── config/
│   ├── config.yaml      # 配置文件
│   └── accounts.yaml    # 公众号列表
├── scripts/
│   ├── single_crawl.py  # 单公众号爬取
│   ├── batch_crawl.py   # 批量爬取
│   └── update_check.py  # 检查更新
├── utils/
│   ├── tools.py         # 工具函数
│   └── logger.py        # 日志管理
├── README.md
└── requirements.txt
```

## 🚀 实现步骤

### 步骤 1：创建项目结构

```bash
mkdir -p wechat_crawler/{data/{raw,processed,db},modules,config,scripts,utils}
```

### 步骤 2：安装依赖

```bash
pip install requests beautifulsoup4 selenium pandas schedule pyyaml
```

### 步骤 3：实现核心功能

1. **公众号信息获取**：通过微信公众号搜索接口获取公众号基本信息
2. **历史文章爬取**：解析公众号历史文章列表，获取所有推送链接
3. **文章内容提取**：爬取单篇文章的完整内容
4. **数据存储**：将爬取的数据存入 SQLite 数据库
5. **定时任务**：设置定期检查更新的任务

### 步骤 4：创建配置文件

**config.yaml**：
```yaml
# 爬虫配置
crawler:
  timeout: 30
  retry_times: 3
  headers: 
    User-Agent: "Mozilla/5.0..."

# 定时任务配置
scheduler:
  check_interval: "1h"  # 每小时检查一次

# 存储配置
storage:
  db_path: "data/db/wechat.db"
  batch_size: 100
```

**accounts.yaml**：
```yaml
# 公众号列表
accounts:
  - name: "公众号名称1"
    id: "gh_xxxxxx"
    last_update: ""
  - name: "公众号名称2"
    id: "gh_yyyyyy"
    last_update: ""
```

### 步骤 5：创建运行脚本

**single_crawl.py**：
```python
# 单公众号爬取脚本
from modules.crawler import WeChatCrawler
from modules.storage import DataStorage

if __name__ == "__main__":
    account_name = input("请输入微信公众号名称：")
    crawler = WeChatCrawler()
    storage = DataStorage()
    
    print(f"开始爬取 '{account_name}' 的所有推送...")
    articles = crawler.crawl_account(account_name)
    storage.save_articles(articles)
    print(f"爬取完成，共获取 {len(articles)} 篇文章")
```

**batch_crawl.py**：
```python
# 批量爬取脚本
from modules.crawler import WeChatCrawler
from modules.storage import DataStorage
from modules.scheduler import Scheduler
import yaml

if __name__ == "__main__":
    # 读取公众号列表
    with open('config/accounts.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    accounts = config.get('accounts', [])
    
    crawler = WeChatCrawler()
    storage = DataStorage()
    scheduler = Scheduler()
    
    # 初始爬取
    for account in accounts:
        print(f"开始爬取 '{account['name']}'...")
        articles = crawler.crawl_account(account['name'])
        storage.save_articles(articles)
        print(f"'{account['name']}' 爬取完成")
    
    # 启动定时任务
    print("启动定时更新任务...")
    scheduler.start_schedule()
```

## 🔧 核心模块设计

### 1. 爬虫模块 (crawler.py)

| 功能 | 方法 | 说明 |
|------|------|------|
| 公众号搜索 | `search_account(name)` | 根据名称搜索公众号 |
| 历史文章获取 | `get_history_articles(account_id)` | 获取公众号所有历史文章 |
| 文章内容提取 | `extract_article_content(url)` | 提取单篇文章完整内容 |
| 新文章检查 | `check_new_articles(account_id, last_update)` | 检查是否有新文章 |

### 2. 存储模块 (storage.py)

| 功能 | 方法 | 说明 |
|------|------|------|
| 数据库初始化 | `init_db()` | 创建数据库表结构 |
| 文章保存 | `save_articles(articles)` | 批量保存文章数据 |
| 文章查询 | `query_articles(account_name, limit)` | 查询指定公众号的文章 |
| 上次更新时间 | `get_last_update(account_id)` | 获取公众号上次更新时间 |
| 导出数据 | `export_to_csv(account_name, filename)` | 将数据导出为 CSV |

### 3. 定时任务模块 (scheduler.py)

| 功能 | 方法 | 说明 |
|------|------|------|
| 任务创建 | `create_schedule()` | 创建定时检查任务 |
| 任务启动 | `start_schedule()` | 启动定时任务 |
| 任务停止 | `stop_schedule()` | 停止定时任务 |
| 更新检查 | `check_updates()` | 检查所有公众号的更新 |

### 4. 图形界面模块 (gui.py) [可选]

| 功能 | 方法 | 说明 |
|------|------|------|
| 主界面 | `MainWindow()` | 创建主窗口 |
| 公众号添加 | `add_account_dialog()` | 添加新公众号对话框 |
| 状态显示 | `update_status(message)` | 更新状态信息 |
| 数据展示 | `show_articles_table()` | 展示爬取的文章 |

## 💾 数据结构

### 数据库表结构

**`accounts` 表**
| 字段 | 类型 | 说明 |
|------|------|------|
| `id` | `INTEGER` | 自增主键 |
| `account_name` | `TEXT` | 公众号名称 |
| `account_id` | `TEXT` | 公众号唯一ID |
| `last_update` | `DATETIME` | 上次更新时间 |
| `created_at` | `DATETIME` | 添加时间 |

**`articles` 表**
| 字段 | 类型 | 说明 |
|------|------|------|
| `id` | `INTEGER` | 自增主键 |
| `account_id` | `TEXT` | 所属公众号ID |
| `title` | `TEXT` | 文章标题 |
| `content` | `TEXT` | 文章内容 |
| `url` | `TEXT` | 文章链接 |
| `publish_time` | `DATETIME` | 发布时间 |
| `cover_image` | `TEXT` | 封面图片URL |
| `reading_count` | `INTEGER` | 阅读数（如果可获取） |
| `like_count` | `INTEGER` | 点赞数（如果可获取） |
| `created_at` | `DATETIME` | 爬取时间 |

## ⚠️ 注意事项

1. **微信 API 限制**：微信有反爬机制，需要合理控制爬取频率
2. **登录验证**：部分接口可能需要微信登录状态
3. **数据一致性**：需要处理重复文章的去重逻辑
4. **错误处理**：需要处理网络错误、解析错误等异常情况
5. **定时任务**：需要考虑程序持续运行的稳定性

## 📊 预期输出

1. **爬取结果**：
   - 公众号基本信息
   - 所有历史文章列表
   - 文章详细内容
   - 发布时间、阅读数等元数据

2. **存储格式**：
   - SQLite 数据库（结构化存储）
   - 可选：CSV/JSON 导出

3. **定期更新**：
   - 自动检查新推送
   - 仅爬取未获取的新文章
   - 发送更新通知（可选）

## 🚀 部署方案

### 本地部署
1. 安装依赖：`pip install -r requirements.txt`
2. 配置公众号列表：编辑 `config/accounts.yaml`
3. 运行批量爬取：`python scripts/batch_crawl.py`

### 服务器部署
1. 使用 `nohup` 或 `screen` 后台运行
2. 配置系统服务，实现开机自启
3. 设置日志轮转，避免日志过大

## 💡 扩展功能

1. **内容分析**：对文章内容进行关键词提取、情感分析
2. **可视化**：生成文章发布频率、阅读量趋势图表
3. **推送通知**：新文章发布时通过邮件/微信通知
4. **多平台支持**：支持今日头条、知乎等其他平台
5. **API 服务**：提供 RESTful API 接口

---

## 🎯 下一步

1. **创建项目目录结构**
2. **实现核心爬虫模块**
3. **创建数据存储模块**
4. **实现定时任务功能**
5. **编写运行脚本**
6. **测试和优化**

项目完成后，用户可以轻松管理多个微信公众号，自动获取所有历史推送，并定期接收新内容的更新。